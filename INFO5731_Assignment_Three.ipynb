{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Three**\n",
    "\n",
    "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Understand N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [],
   "source": [
    "#(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "#(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "#(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "#(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets).\n",
    "\n",
    "\n",
    "dat = open('data/reviews.txt', 'r')\n",
    "data = dat.readlines()\n",
    "documents = []\n",
    "\n",
    "query = input('Enter a query: ')\n",
    "\n",
    "def extract_noun_phrases(data):\n",
    "    \"\"\"\n",
    "    Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets).\n",
    "    \"\"\"\n",
    "    # Initialize the noun phrase dictionary\n",
    "    noun_phrase_dict = {}\n",
    "    # Loop through the data\n",
    "    for i in range(len(data)):\n",
    "        # Split the data into words\n",
    "        words = data[i].split()\n",
    "        # Loop through the words\n",
    "        for j in range(len(words)):\n",
    "            # Get the word\n",
    "            word = words[j]\n",
    "            # Check if the word is in the noun phrase dictionary\n",
    "            if word in noun_phrase_dict:\n",
    "                # If it is, increase the frequency\n",
    "                noun_phrase_dict[word] += 1\n",
    "            else:\n",
    "                # If it is not, add it to the dictionary\n",
    "                noun_phrase_dict[word] = 1\n",
    "    # Return the noun phrase dictionary\n",
    "    return noun_phrase_dict\n",
    "\n",
    "extract_noun_phrases(data=data)\n",
    "\n",
    "def count_ngram_frequency(data, n):\n",
    "    \"\"\"\n",
    "    Count the frequency of all the N-grams (N=3).\n",
    "    \"\"\"\n",
    "    # Initialize the frequency dictionary\n",
    "    freq_dict = {}\n",
    "    # Loop through the data\n",
    "    for i in range(len(data)):\n",
    "        # Split the data into words\n",
    "        words = data[i].split()\n",
    "        # Loop through the words\n",
    "        for j in range(len(words) - n + 1):\n",
    "            # Get the n-gram\n",
    "            ngram = ' '.join(words[j:j+n])\n",
    "            # Check if the n-gram is in the dictionary\n",
    "            if ngram in freq_dict:\n",
    "                # If it is, increase the frequency\n",
    "                freq_dict[ngram] += 1\n",
    "            else:\n",
    "                # If it is not, add it to the dictionary\n",
    "                freq_dict[ngram] = 1\n",
    "    # Return the frequency dictionary\n",
    "    return freq_dict\n",
    "count_ngram_frequency(data=data, n=3)\n",
    "\n",
    "def calculate_probability_from_all_dataset(bigram_freq_dict, n):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "    \"\"\"\n",
    "    # Initialize the probability dictionary\n",
    "    prob_dict = {}\n",
    "    # Loop through the bigram frequency dictionary\n",
    "    for key in bigram_freq_dict:\n",
    "        # Get the frequency of the bigram\n",
    "        freq = bigram_freq_dict[key]\n",
    "        # Split the bigram into words\n",
    "        words = key.split()\n",
    "        # Get the first word\n",
    "        w1 = words[0]\n",
    "        # Get the second word\n",
    "        w2 = words[1]\n",
    "        # Check if the first word is in the probability dictionary\n",
    "        if w1 in prob_dict:\n",
    "            # If it is, add the second word and the frequency to the dictionary\n",
    "            prob_dict[w1][w2] = freq\n",
    "        else:\n",
    "            # If it is not, add the first word and the second word and the frequency to the dictionary\n",
    "            prob_dict[w1] = {w2: freq}\n",
    "    # Return the probability dictionary\n",
    "    return prob_dict\n",
    "calculate_probability_from_all_dataset(bigram_freq_dict=data, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Undersand TF-IDF and Document representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
    "\n",
    "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [],
   "source": [
    "#Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
    "\n",
    "#(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "#(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**.\n",
    "\n",
    "\n",
    "def build_documents_terms_weight_tfidf(documents, terms):\n",
    "    \"\"\"\n",
    "    Build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "    \"\"\"\n",
    "    # Initialize the documents-terms weights dictionary\n",
    "    documents_terms_weights = {}\n",
    "    # Loop through the documents\n",
    "    for i in range(len(documents)):\n",
    "        # Split the document into words\n",
    "        words = documents[i].split()\n",
    "        # Loop through the words\n",
    "        for j in range(len(words)):\n",
    "            # Get the word\n",
    "            word = words[j]\n",
    "            # Check if the word is in the documents-terms weights dictionary\n",
    "            if word in documents_terms_weights:\n",
    "                # If it is, increase the frequency\n",
    "                documents_terms_weights[word][i] += 1\n",
    "            else:\n",
    "                # If it is not, add it to the dictionary\n",
    "                documents_terms_weights[word] = {i: 1}\n",
    "    # Return the documents-terms weights dictionary\n",
    "    return documents_terms_weights\n",
    "\n",
    "\n",
    "\n",
    "def rank_the_document_tfidf(documents, query):\n",
    "    \"\"\"\n",
    "    Rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    # Initialize the documents-terms weights dictionary\n",
    "    documents_terms_weights = {}\n",
    "    # Split the query into words\n",
    "    words = query.split()\n",
    "    # Loop through the words\n",
    "    for j in range(len(words)):\n",
    "        # Get the word\n",
    "        word = words[j]\n",
    "        # Check if the word is in the documents-terms weights dictionary\n",
    "        if word in documents_terms_weights:\n",
    "            # If it is, increase the frequency\n",
    "            documents_terms_weights[word][i] += 1\n",
    "        else:\n",
    "            # If it is not, add it to the dictionary\n",
    "            documents_terms_weights[word] = {i: 1}\n",
    "    # Return the documents-terms weights dictionary\n",
    "    return documents_terms_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question 3: Create your own word embedding model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
    "\n",
    "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
    "\n",
    "(2) Visualize the word embedding model you created.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#(20 points). Use the data you collected for assignment two to build a word embedding model:\n",
    "\n",
    "#(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
    "\n",
    "#(2) Visualize the word embedding model you created.\n",
    "\n",
    "#Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "def train_300_dimension_word_embedding(documents, query):\n",
    "    \"\"\"\n",
    "    Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
    "    \"\"\"\n",
    "     \"\"\"\n",
    "    Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
    "    \"\"\"\n",
    "    # Initialize the word2vec model\n",
    "    model = word2vec.Word2Vec(documents, size=300, window=5, min_count=5, workers=4)\n",
    "    # Return the word2vec model\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        print(model.wv.index2word[i])\n",
    "    i = 0\n",
    "    # Initialize the documents-terms weights dictionary\n",
    "    documents_terms_weights = {}\n",
    "    # Split the query into words\n",
    "    words = query.split()\n",
    "    # Loop through the words\n",
    "    for j in range(len(words)):\n",
    "        # Get the word\n",
    "        word = words[j]\n",
    "        # Check if the word is in the documents-terms weights dictionary\n",
    "        if word in documents_terms_weights:\n",
    "            # If it is, increase the frequency\n",
    "            documents_terms_weights[word][i] += 1\n",
    "        else:\n",
    "            # If it is not, add it to the dictionary\n",
    "            documents_terms_weights[word] = {i: 1}\n",
    "    # Return the documents-terms weights dictionary\n",
    "    return documents_terms_weights\n",
    "\n",
    "def visualize_word_embedding(documents, query):\n",
    "    \"\"\"\n",
    "    Visualize the word embedding model you created.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    # Initialize the documents-terms weights dictionary\n",
    "    documents_terms_weights = {}\n",
    "    # Split the query into words\n",
    "    words = query.split()\n",
    "    # Loop through the words\n",
    "    for j in range(len(words)):\n",
    "        # Get the word\n",
    "        word = words[j]\n",
    "        # Check if the word is in the documents-terms weights dictionary\n",
    "        if word in documents_terms_weights:\n",
    "            # If it is, increase the frequency\n",
    "            documents_terms_weights[word][i] += 1\n",
    "        else:\n",
    "            # If it is not, add it to the dictionary\n",
    "            documents_terms_weights[word] = {i: 1}\n",
    "    # Return the documents-terms weights dictionary\n",
    "    return documents_terms_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# The GitHub link of your final csv file\n",
    "\n",
    "#(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotat  `ed dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification.\n",
    "\n",
    "\n",
    "\n",
    "# Link: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMniIalS+f3MyeuLTJeFDvi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
